---
title: "systemPipe: Workflow and Visualization Toolkit"
author: ""
date: "Last update: `r format(Sys.time(), '%d %B, %Y')`" 
output:
  BiocStyle::html_document:
    toc_float: true
    code_folding: show
vignette: |
  %\VignetteEncoding{UTF-8}
  %\VignetteIndexEntry{systemPipe: Workflow and Visualization Toolkit}
  %\VignetteEngine{knitr::rmarkdown}
fontsize: 14pt
editor_options: 
  markdown: 
    wrap: 80
  chunk_output_type: console
bibliography: bibtex.bib
---

```{r setting, echo=FALSE}
if (file.exists("bioc2021")) unlink("bioc2021", recursive = TRUE)
```

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

**Authors**:
    Daniela Cassol (danielac@ucr.edu),
    Le Zhang (le.zhang001@email.ucr.edu),
    Thomas Girke (thomas.girke@ucr.edu).
    
**Institution**: Institute for Integrative Genome Biology, University of California, Riverside, California, USA.

# Overview

## Workshop Description

This workshop introduces _systemPipe_ (SP), a generic toolkit for designing and running reproducible data analysis workflows. The environment consists of three major modules implemented as R/Bioconductor packages. _systemPipeR_ (SPR) provides core functionalities for defining workflows, interacting with command-line software, and executing both R and/or command-line software, as well as generating publication-quality analysis reports. _systemPipeShiny_ (SPS) integrates a graphical user interface for managing workflows and visualizing results interactively. _systemPipeWorkflow_ (SPW) offers a collection of pre-configured workflow templates. This hands-on event will include the following topics: (1) brief overview of the design principles and functionalities of the SP toolkit; (2) design and usage of SPR's command-line interface based on an object-oriented R implementation of CWL; (3) configuration and execution of workflows; (4) construction of custom workflows; (5) configuration and execution of a pre-configured workflow example from start to finish, e.g. RNA-Seq template; (6) parallel execution of workflows on HPC and cloud systems with and without schedulers; (7) generation of technical and scientific analysis reports including visualization; and 
(8) demonstration of SPS' core functionalities, the project's Shiny App.

## Pre-requisites

  * Basic knowledge of R and usage of Bioconductor packages for NGS analysis
  * Basic knowledge of running command-line software
  * Basic knowledge of parallelization concepts

Non-essential background reading:

  * [systemPipe Workflow Environment WebPage](https://systempipe.org/)
  * [systemPipeR vignette](https://bioconductor.org/packages/devel/bioc/vignettes/systemPipeR/inst/doc/systemPipeR.html)
  * [systemPipeShiny vignette](https://bioconductor.org/packages/devel/bioc/vignettes/systemPipeShiny/inst/doc/systemPipeShiny.html)
  * [systemPipeRdata vignette](https://bioconductor.org/packages/release/data/experiment/vignettes/systemPipeRdata/inst/doc/systemPipeRdata.html)
  * [R Markdown tutorial](https://rmarkdown.rstudio.com/lesson-2.html)

## Workshop Participation

Participants will be able to perform all analysis components of this workshop hands-on. Active user participation throughout the event is highly encouraged, including but not limited to lecture material, hands-on sections, and final discussion about package improvements. Participants are encouraged to ask questions preferentially at the end of the workshop.

## _R_ / _Bioconductor_ packages used

* [`systemPipeR`](http://www.bioconductor.org/packages/release/bioc/html/systemPipeR.html)
* [`systemPipeShiny`](https://bioconductor.org/packages/devel/bioc/html/systemPipeShiny.html)
* [`systemPipeRdata`](http://www.bioconductor.org/packages/release/data/experiment/html/systemPipeRdata.html)

## Time outline

1h 45m total

| Activity                                                         | Time |
|------------------------------------------------------------------|------|
| Overview of *systemPipe* toolkit                                 | 10m  | 
| Introduction to SPR's command-line interface                     | 20m  |
| Configuration and execution of workflows                         | 20m  |
| Showcase RNA-Seq workflow                                        | 20m  |
| Parallelization on single machines and clusters                  | 10m  |
| Generation of technical and scientific analysis reports          | 5m   |
| Overview of *systemPipeShiny* core functionalities               | 20m  |

## Workshop goals and objectives

### Learning goals

* Recognize the benefits of a generic R-based workflow construction environment that is both scalable and reproducible
* Integration of command-line tools via the CWL community standard
* Rendering of R markdown reports and critical assessment of scientific analysis reports
* Parallelization of big data analysis tasks

### Learning objectives

* Identify and practice how to make analysis workflows more robust, reproducible, and portable across heterogeneous computing systems
* Usage of new workflow control class for designing, configuring, and running workflows
* Optimize and debug workflows
* Inspection of technical reports and log files
* Design of new and fully customized workflows
* Practice interactive workflow management and visualization 

# Workshop

## Running the Workshop 

This workshop uses R `4.1.0` and Bioconductor version`3.14`. Bioconductor can be
installed following [these instructions](https://www.bioconductor.org/developers/how-to/useDevel/).

During the [Bioc2021 conference](https://bioc2021.bioconductor.org/), the workshop can be run in the [cloud](http://app.orchestra.cancerdatasci.org/).

### Workshop setup with Docker

The Docker container used by this workshop runs with Bioconductor's `development` version `3.14`. I includes 
all the necessary packages and software for running the code of in the workshop vignettes. 
To use the Docker container, one needs to first install [Docker](https://docs.docker.com/engine/install/) on a user's system.

- The container can be downloaded and run with:

```{bash docker, eval=FALSE}
docker run -e PASSWORD=systempipe -p 8787:8787 systempipe/systempipeworkshop2021:latest
```

- Log in to RStudio at [http://localhost:8787](http://localhost:8787) using username `rstudio`
and password `systempipe`.

- If you prefer to run the workshop from the command-line:

```{bash docker_bash, eval=FALSE}
docker run -it --user rstudio systempipe/systempipeworkshop2021:latest bash 
```

### Workshop setup with GitHub

[_`systemPipeR`_](http://www.bioconductor.org/packages/devel/bioc/html/systemPipeR.html) and
[_`systemPipeShiny`_](http://bioconductor.org/packages/devel/bioc/html/systemPipeShiny.html)
environment can be installed from the R console using the [_`BiocManager::install`_](https://cran.r-project.org/web/packages/BiocManager/index.html) 
command. The associated data package [_`systemPipeRdata`_](http://www.bioconductor.org/packages/devel/data/experiment/html/systemPipeRdata.html) 
can be installed the same way. The latter is a helper package for generating _`systemPipeR`_ 
workflow environments with a single command containing all parameter files and 
sample data required to quickly test and run workflows. 

To install all packages requried for this workshop on a local system, one can use the following 
install commands. 

```{r, install_pkg, eval=FALSE}
## Install workshop package
BiocManager::install("systemPipeR/systemPipeWorkshop2021")
## Install required packages
BiocManager::install(c("systemPipeR", "systemPipeRdata", "systemPipeShiny"), version="3.14")
```

To access the vignette:

```{r, vignette, eval=FALSE}
browseVignettes(package = "systemPipeWorkshop2021")
```

## Getting Started

### Loading package and documentation

```{r documentation, eval=TRUE, message=FALSE}
library("systemPipeR") 
library("systemPipeShiny") 
library("systemPipeRdata") 
```

- Documentation

```{r library, eval=FALSE}
library(help="systemPipeR") # Lists package info
vignette("systemPipeR") # Opens vignette
```

### Set working directory

```{r setwd, eval=FALSE}
setwd("vignettes")
```

### How to find help 

All questions about the package or any particular function should be posted to 
the Bioconductor support site [https://support.bioconductor.org](https://support.bioconductor.org). 
Please add the "_`systemPipeR`_" tag to your question. This triggers an email 
alert that will be send to the authors.
We also appreciate receiving suggestions for improvements and/or bug reports by opening issues 
on [GitHub](https://github.com/systemPipeR). 

## Overview of *systemPipe* toolkit

`systemPipe` (SP) is a generic toolkit for designing and running reproducible data
analysis workflows. The environment consists of three major modules implemented 
as R/Bioconductor packages:

  - `systemPipeR` (SPR) provides core functionalities for defining workflows, 
    interacting with command-line software, and executing both R and/or command-line
    software, as well as generating publication-quality analysis reports.
 
  - `systemPipeShiny` (SPS) integrates a graphical user interface for managing 
    workflows and visualizing results interactively. 
    
  - `systemPipeWorkflow` (SPW) offers a collection of pre-configured workflow templates. 
    
## Introduction to SPR's command-line interface

A central concept for designing workflows within the `systemPipeR` environment is 
the use of workflow management containers. `systemPipeR` adopted the widely used 
community standard [Common Workflow Language](https://www.commonwl.org/) (CWL) 
[@Amstutz2016-ka] for describing analysis workflows in a generic and reproducible 
manner.
Using this community standard in `systemPipeR` has many advantages. For instance, 
the integration of CWL allows running `systemPipeR` workflows from a single 
specification instance either entirely from within R, from various command-line 
wrappers (e.g. cwl-runner) or from other languages (e.g. Bash or Python). 
`systemPipeR` includes support for both command-line and R/Bioconductor software 
as well as resources for containerization, parallel evaluations on computer 
clusters along with automated generation of interactive analysis reports.

An important feature of `systemPipeR's` CWL interface is that it provides two 
options to run command-line tools and workflows based on CWL. 
First, one can run CWL in its native way via an R-based wrapper utility for 
`cwl-runner` or `cwl-tools` (CWL-based approach). Second, one can run workflows 
using CWL's command-line and workflow instructions from within R (R-based approach). 
In the latter case the same CWL workflow definition files (e.g. *.cwl* and *.yml*) 
are used but rendered and executed entirely with R functions defined by `systemPipeR`, 
and thus use CWL mainly as a command-line and workflow definition format rather 
than software to run workflows. Moreover, `systemPipeR` provides several 
convenience functions that are useful for designing and debugging workflows, 
such as a command-line rendering function to retrieve the exact command-line 
strings for each step prior to running a command-line.

## Configuration and execution of workflows

### Load sample data and directory structure

```{r lib, eval=TRUE}
systemPipeRdata::genWorkenvir("new", mydirname = "bioc2021")
```

```{r setting_dir, include=FALSE, warning=FALSE}
setwd("bioc2021")
knitr::opts_knit$set(root.dir = 'bioc2021')
```

### Project structure

_`systemPipeR`_ expects a project directory structure that consists of a directory
where users may store all the raw data, the results directory that will be reserved 
for all the outfiles files or new output folders, and the parameters directory. 

This structure allows reproducibility and collaboration across the data science 
team since internally relative paths are used. Users could transfer this project 
to a different location and still be able to run the entire workflow. Also, it 
increases efficiency and data management once the raw data is kept in a separate 
folder and avoids duplication. 

<p style="color:red">[ThG Comment: The above is a bit fuzzy. I would merge anything relevent with the 
following subsection. Keep it short and informative and eliminate the verbose parts
that are not necessary.]</p>

#### Directory Structure {#dir}

[_`systemPipeRdata`_](http://bioconductor.org/packages/devel/data/experiment/html/systemPipeRdata.html), 
helper package, provides pre-configured workflows, reporting 
templates, and sample data loaded as demonstrated below. With a single command, 
the package allows creating the workflow environment containing the structure 
described here (see Figure \@ref(fig:dir)).

Directory names are indicated in <span style="color:grey">***green***</span>.
Users can change this structure as needed, but need to adjust the code in their 
workflows accordingly. 

* <span style="color:green">_**workflow/**_</span> (*e.g.* *myproject/*) 
    + This is the root directory of the R session running the workflow.
    + Run script ( *\*.Rmd*) and sample annotation (*targets.txt*) files are located here.
    + Note, this directory can have any name (*e.g.* <span style="color:green">_**myproject**_</span>). Changing its name does not require any modifications in the run script(s).
  + **Important subdirectories**: 
    + <span style="color:green">_**param/**_</span> 
        + <span style="color:green">_**param/cwl/**_</span>: This subdirectory stores all the parameter and configuration files. To organize workflows, each can have its own subdirectory, where all `*.cwl` and `*input.yml` files need to be in the same subdirectory. 
    + <span style="color:green">_**data/**_ </span>
        + Raw data (*e.g.* FASTQ files)
        + FASTA file of reference (*e.g.* reference genome)
        + Annotation files
        + Metadata
        + etc.
    + <span style="color:green">_**results/**_</span>
        + Analysis results are usually written to this directory, including: alignment, variant and peak files (BAM, VCF, BED); tabular result files; and image/plot files
        + Note, the user has the option to organize results files for a given sample and analysis step in a separate subdirectory.

```{r dir, eval=TRUE, echo=FALSE, out.width="100%", fig.align = "center", fig.cap= "*systemPipeR's* preconfigured directory structure."}
knitr::include_graphics(system.file("images", "spr_project.png", package = "systemPipeWorkshop2021"))  
```

#### Structure of initial _`targets`_ file

The _`targets`_ file defines all input files (_e.g._ FASTQ, BAM, BCF) and sample 
comparisons of an analysis workflow. The following shows the format of a sample 
_`targets`_ file included in the package. It also can be viewed and downloaded 
from _`systemPipeR`_'s GitHub repository [here](https://github.com/tgirke/systemPipeR/blob/master/inst/extdata/targets.txt). 
In a target file with a single type of input files, here FASTQ files of 
single-end (SE) reads, the first column defines the paths and the second column
represents a unique id name for each sample. The third column called `Factor` 
represents the biological replicates. All subsequent columns are optional to provide
additional information. Any number of additional columns can be added as needed.

Users should note here, the usage of targets files is optional when using
_`systemPipeR's`_ new workflow management interface. They can be replaced by a standard YAML
input file used by CWL. Since for organizing experimental variables targets
files are extremely useful and user-friendly. Thus, we encourage users to keep using 
them. 

- Structure of _`targets`_ file for single-end (SE) samples

```{r targetsSE, eval=TRUE}
targetspath <- "targets.txt"
showDF(read.delim(targetspath, comment.char = "#"))
```

To work with custom data, users need to generate a _`targets`_ file containing 
the paths to their own FASTQ files and then provide under _`targetspath`_ the
path to the corresponding _`targets`_ file. 

- Structure of _`targets`_ file for "Hello World" example

In this example, the _`targets`_ file contains only two columns. The first column
contains short text strings that will be used by the `echo` command-line. The second column
contains sample `ids`. The `id` column is required, and each sample `id` should be unique. 

<p style="color:red">[ThG Comment: it is confusing that the text states as column title id 
while the example below clearly has a different title, here SampleName. Try to be consistent...]</p>

```{r targets_echo, eval=TRUE}
targetspath <- system.file("extdata/cwl/example/targets_example.txt", package = "systemPipeR")
showDF(read.delim(targetspath, comment.char = "#"))
```

### Structure of parameters files

The parameters required for running command-line software are provided by adopting the 
widely used CWL ([Common Workflow Language](https://www.commonwl.org/)) community standard 
[@Amstutz2016-ka]. Parameter files are only required for command-line steps, and for R-based workflow 
they are optional. An overview of the CWL syntax is provided in the [section](#cwl) below, while
the [here](#cwl_targets) section explains how target files can be used for CWL-based workflow
steps.

#### Automate creation of CWL parameters

Users need to define the command-line in a pseudo-bash script format:

```{r cmd, eval=TRUE}
# "hisat2 -S ./results/M1A.sam -x ./data/tair10.fasta -k 1 -threads 4 -U ./data/SRR446027_1.fastq.gz "
command <- "
    hisat2 \
    -S <F, out: ./results/M1A.sam> \
    -x <F: ./data/tair10.fasta> \
     -k <int: 1> \
    -threads <int: 4> \
    -U <F: ./data/SRR446027_1.fastq.gz>
"
```

##### Define prefix and defaults

- First line is the base command. Each line is an argument with its default value.

- For argument lines (starting from the second line), any word before the first 
  space with leading `-` or `--` in each will be treated as a prefix, like `-S` or 
  `--min`. Any line without this first word will be treated as no prefix. 
  
- All defaults are placed inside `<...>`.

- First argument is the input argument type. `F` for "File", "int", "string" are unchanged.

- Optional: use the keyword `out` followed the type with a `,` comma separation to 
  indicate if this argument is also an CWL output.
  
- Then, use `:` to separate keywords and default values, any non-space value after the `:`
  will be treated as the default value. 
  
- If any argument has no default value, just a flag, like `--verbose`, there is no need to add any `<...>`

##### `createParam` Function

`createParam` function requires the `string` as defined above as an input. 

First of all, the function will print the three components of the `cwl` file:
    - `BaseCommand`: Specifies the program to execute. 
    - `Inputs`: Defines the input parameters of the process.
    - `Outputs`: Defines the parameters representing the output of the process.
    
The four component is the original command-line.

If in interactive mode, the function will verify that everything is correct and 
will ask you to proceed. Here, the user can answer "no" and provide more 
information at the string level. Another question is to save the param created here.

If running the workflow in non-interactive mode, the `createParam` function will 
consider "yes" and returning the container.

```{r}
cmd <- createParam(command, writeParamFiles = TRUE)
```

#### How to connect CWL description files within _`systemPipeR`_ {#cwl_targets}

This section will demonstrate how to connect CWL parameters files to create 
workflows. In addition, we will show how the workflow can be easily scalable 
with _`systemPipeR`_.

```{r sprCWL, eval=TRUE, echo=FALSE, out.width="100%", fig.align = "center", fig.cap= "WConnectivity between CWL param files and targets files."}
knitr::include_graphics(system.file("images", "SPR_CWL_hello.png", package = "systemPipeWorkshop2021"))
```

## Project initialization

To create a Workflow within _`systemPipeR`_, we can start by defining an empty
container and checking the directory structure:

```{r SPRproject, eval=TRUE}
sal <- SPRproject(projPath = getwd()) 
```

Internally, `SPRproject` function will create a hidden folder called `.SPRproject`,
by default, to store all the log files.
A `YAML` file, here called `SYSargsList.yml`, has been created, which initially
contains the basic location of the project structure; however, every time the 
workflow object `sal` is updated in R, the new information will also be store in this 
flat-file database for easy recovery.
If you desire different names for the logs folder and the `YAML` file, these can 
be modified as follows:

```{r SPRproject_logs, eval=FALSE}
sal <- SPRproject(logs.dir= ".SPRproject", sys.file=".SPRproject/SYSargsList.yml") 
```

Also, this function will check and/or create the basic folder structure if missing, 
which means `data`, `param`, and `results` folder, as described [here](#dir). 
If the user wants to use a different names for these directories, can be specified 
as follows:

```{r SPRproject_dir, eval=FALSE}
sal <- SPRproject(data = "data", param = "param", results = "results") 
```

It is possible to separate all the R objects created within the workflow analysis 
from the current environment. `SPRproject` function provides the option to create 
a new environment, and in this way, it is not overwriting any object you may want
to have at your current section. 

```{r SPRproject_env, eval=FALSE}
sal <- SPRproject(envir = new.env()) 
```

In this stage, the object `sal` is a empty container, except for the project information. The project information can be accessed by the `projectInfo` method:

```{r projectInfo, eval=TRUE}
sal
projectInfo(sal)
```

Also, the `length` function will return how many steps this workflow contains and
in this case it is empty, as follow:

```{r length, eval=TRUE}
length(sal)
```

## Workflow Design 

_`systemPipeR`_ workflows can be designed and built from start to finish with a 
single command, importing from an R Markdown file or stepwise in interactive 
mode from the R console. 
In the [next section](#appendstep), we will demonstrate how to build the workflow in an
interactive mode, and in the [following section](#importWF), we will show how to build from a 
file. 

New workflows are constructed, or existing ones modified, by connecting each 
step via `appendStep` method. Each `SYSargsList` instance contains instructions 
needed for processing a set of input files with a specific command-line or R 
software, as well as the paths to the corresponding outfiles generated by a 
particular tool/step. 

To build R code based step, the constructor function `Linewise` is used. 
For more details about this S4 class container, see [here](#linewise). 

### Build workflow interactive {#appendstep}

This tutorial shows a very simple example for describing and explaining all main 
features available within systemPipeR to design, build, manage, run, and 
visualize the workflow. In summary, we are exporting a dataset to multiple 
files, compressing and decompressing each one of the files, and importing to R, 
and finally performing a statistical analysis. 

In the previous section, we initialize the project by building the `sal` object.
Until this moment, the container has no steps:

```{r sal_check, eval=TRUE}
sal
```

Next, we need to populate the object created with the first step in the
workflow.

#### Adding the first step 

The first step is R code based, and we are splitting the `iris` dataset by `Species`
and for each `Species` will be saved on file. Please note that this code will
not be executed now; it is just store in the container for further execution. 

This constructor function requires the `step_name` and the R-based code under 
the `code` argument. 
The R code should be enclosed by braces (`{}`) and separated by a new line. 

```{r, firstStep_R, eval=TRUE}
appendStep(sal) <- LineWise(code = {
                              mapply(function(x, y) write.csv(x, y),
                                     split(iris, factor(iris$Species)),
                                     file.path("results", paste0(names(split(iris, factor(iris$Species))), ".csv"))
                                     ) 
                            },
                            step_name = "export_iris")
```

For a brief overview of the workflow, we can check the object as follows:

```{r show, eval=TRUE}
sal
```

Also, for printing and double-check the R code in the step, we can use the 
`codeLine` method:

```{r codeLine, eval=TRUE}
codeLine(sal)
```

#### Adding more steps

Next, an example of how to compress the exported files using 
[`gzip`](https://www.gnu.org/software/gzip/) command-line. 

The constructor function creates an `SYSargsList` S4 class object using data from
three input files:

    - CWL command-line specification file (`wf_file` argument);
    - Input variables (`input_file` argument);
    - Targets file (`targets` argument).

In CWL, files with the extension `.cwl` define the parameters of a chosen
command-line step or workflow, while files with the extension `.yml` define the
input variables of command-line steps. 

The `targets` file is optional for workflow steps lacking `input` files. The connection 
between `input` variables and the `targets` file is defined under the `inputvars` 
argument. It is required a `named vector`, where each element name needs to match
with column names in the `targets` file, and the value must match the names of 
the `input` variables defined in the `*.yml` files (see Figure \@ref(fig:sprCWL)). 

A detailed description of the dynamic between `input` variables and `targets` 
files can be found [here](#cwl_targets). 
In addition, the CWL syntax overview can be found [here](#cwl). 

Besides all the data form `targets`, `wf_file`, `input_file` and `dir_path` arguments,
`SYSargsList` constructor function options include: 

  - `step_name`: a unique *name* for the step. This is not mandatory; however, 
    it is highly recommended. If no name is provided, a default `step_x`, where
    `x` reflects the step index, will be added. 
  - `dir`: this option allows creating an exclusive subdirectory for the step 
    in the workflow. All the outfiles and log files for this particular step will 
    be generated in the respective folders.
  - `dependency`: after the first step, all the additional steps appended to 
    the workflow require the information of the dependency tree. 

The `appendStep<-` method is used to append a new step in the workflow.

```{r gzip_secondStep, eval=TRUE}
targetspath <- system.file("extdata/cwl/gunzip", "targets_gunzip.txt", package = "systemPipeR")
appendStep(sal) <- SYSargsList(step_name = "gzip", 
                      targets = targetspath, dir = TRUE,
                      wf_file = "gunzip/workflow_gzip.cwl", input_file = "gunzip/gzip.yml",
                      dir_path = system.file("extdata/cwl", package = "systemPipeR"),
                      inputvars = c(FileName = "_FILE_PATH_", SampleName = "_SampleName_"), 
                      dependency = "export_iris")
```

Note: This will not work if the `gzip` is not available on your system 
(installed and exported to PATH) and may only work on Windows systems using PowerShell. 

For a overview of the workflow, we can check the object as follows:

```{r}
sal
```

Note that we have two steps, and it is expected three files from the second step.
Also, the workflow status is *Pending*, which means the workflow object is 
rendered in R; however, we did not execute the workflow yet. 
In addition to this summary, it can be observed this step has three command-lines. 

For more details about the command-line rendered for each target file, it can be 
checked as follows: 

```{r}
cmdlist(sal, step="gzip")
```

#### Using the `outfiles` for the next step

For building this step, all the previous procedures are being used to append the 
next step. However, here, we can observe power features that build the 
connectivity between steps in the workflow.

In this example, we would like to use the outfiles from *gzip* Step, as
input from the next step, which is the *gunzip*. In this case, let's look at the 
outfiles from the first step:

```{r}
outfiles(sal)
```

The column we want to use is "gzip_file". For the argument `targets` in the 
`SYSargsList` function, it should provide the name of the correspondent step in
the Workflow and which `outfiles` you would like to be incorporated in the next 
step. 
The argument `inputvars` allows the connectivity between `outfiles` and the 
new `targets` file. Here, the name of the previous `outfiles` should be provided 
it. Please note that all `outfiles` column names must be unique.

It is possible to keep all the original columns from the `targets` files or remove
some columns for a clean `targets` file.
The argument `rm_targets_col` provides this flexibility, where it is possible to
specify the names of the columns that should be removed. If no names are passing
here, the new columns will be appended. 

```{r gunzip, eval=TRUE}
appendStep(sal) <- SYSargsList(step_name = "gunzip", 
                      targets = "gzip", dir = TRUE,
                      wf_file = "gunzip/workflow_gunzip.cwl", input_file = "gunzip/gunzip.yml",
                      dir_path = system.file("extdata/cwl", package = "systemPipeR"),
                      inputvars = c(gzip_file = "_FILE_PATH_", SampleName = "_SampleName_"), 
                      rm_targets_col = "FileName", 
                      dependency = "gzip")
```

We can check the targets automatically create for this step, 
based on the previous `outfiles`:

```{r targetsWF_3, eval=TRUE}
targetsWF(sal[3])
```

We can also check all the expected `outfiles` for this particular step, as follows:

```{r outfiles_2, eval=TRUE}
outfiles(sal[3])
```

Now, we can observe that the third step has been added and contains one substep.

```{r}
sal
```

In addition, we can access all the command-lines for each one of the substeps. 

```{r, eval=TRUE}
cmdlist(sal["gzip"], targets = 1)
```

#### Getting data from a workflow instance 

The final step in this simple workflow is an R code step. For that, we are using
the `LineWise` constructor function as demonstrated above. 

One interesting feature showed here is the `getColumn` method that allows 
extracting the information for a workflow instance. Those files can be used in
an R code, as demonstrated below. 

```{r getColumn, eval=TRUE}
getColumn(sal, step = "gunzip", 'outfiles')
```

```{r, iris_stats, eval=TRUE}
appendStep(sal) <- LineWise(code = {
                    df <- lapply(getColumn(sal, step = "gunzip", 'outfiles'), function(x) read.delim(x, sep = ",")[-1])
                    df <- do.call(rbind, df)
                    stats <- data.frame(cbind(mean = apply(df[,1:4], 2, mean), sd = apply(df[,1:4], 2, sd)))
                    stats$species <- rownames(stats)
                    
                    plot <- ggplot2::ggplot(stats, ggplot2::aes(x = species, y = mean, fill = species)) + 
                      ggplot2::geom_bar(stat = "identity", color = "black", position = ggplot2::position_dodge()) +
                      ggplot2::geom_errorbar(ggplot2::aes(ymin = mean-sd, ymax = mean+sd), width = .2, position = ggplot2::position_dodge(.9)) 
                    },
                    step_name = "iris_stats", 
                    dependency = "gzip")
```

### Build workflow from a {R Markdown} {#importWF}

The precisely same workflow can be created by importing the steps from an 
R Markdown file.
As demonstrated above, it is required to initialize the project with `SPRproject` function. 

`importWF` function will scan and import all the R chunk from the R Markdown file 
and build all the workflow instances. Then, each R chuck in the file will be 
converted in a workflow step. 

```{r importWF_rmd, eval=TRUE}
sal_rmd <- SPRproject(logs.dir = ".SPRproject_rmd") 

sal_rmd <- importWF(sal_rmd, 
                file_path = system.file("extdata", "spr_simple_wf.Rmd", package = "systemPipeR"))
```

Let's explore the workflow to check the steps:

```{r importWF_details}
stepsWF(sal_rmd)
dependency(sal_rmd)
codeLine(sal_rmd)
targetsWF(sal_rmd)
```

#### Rules to create the R Markdown to import as workflow

To include a particular code chunk from the R Markdown file in the workflow
analysis, please use the following code chunk options:

    -   `spr='r'`: for code chunks with R code lines;
    -   `spr='sysargs'`: for code chunks with an `SYSargsList` object;
    -   `spr.dep=<StepName>`: for specify the previous dependency.

For example: 

> *```{r step_1, eval=TRUE, spr='r', spr.dep='step_0'}*

> *```{r step_2, eval=TRUE, spr='sysargs', spr.dep='step_1'}*

For `spr = 'sysargs'`, the last object assigned must to be the `SYSargsList`, for example:

```{r fromFile_example_rules, eval=TRUE, spr='sysargs'}
targetspath <- system.file("extdata/cwl/example/targets_example.txt", package = "systemPipeR")
HW_mul <- SYSargsList(step_name = "Example", 
                      targets = targetspath, 
                      wf_file = "example/example.cwl", input_file = "example/example.yml", 
                      dir_path = system.file("extdata/cwl", package = "systemPipeR"), 
                      inputvars = c(Message = "_STRING_", SampleName = "_SAMPLE_"))
```

Also, note that all the required files or objects to generate one particular 
command-line step must be defined in a R code chunk imported. 
The motivation for this is that when R Markdown files are imported, the 
`spr = 'sysargs'` R chunk will be evaluated and stored in the workflow control 
class as the `SYSargsList` object, while the R code based (`spr = 'r'`) is not 
evaluated, and until the workflow is executed it will be store as an expression.

## Running the workflow

For running the workflow, `runWF` function will execute all the command-lines 
store in the workflow container.

```{r runWF, eval=TRUE}
sal <- runWF(sal)
```

This essential function allows the user to choose one or multiple steps to be 
executed using the `steps` argument. However, it is necessary to follow the 
workflow dependency graph. If a selected step depends on a previous step(s) that
was not executed, the execution will fail. 

```{r runWF_error, eval=FALSE}
sal <- runWF(sal, steps = c(1,3))
```

Also, it allows forcing the execution of the steps, even if the status of the 
step is `'Success'` and all the expected `outfiles` exists. 
Another feature of the `runWF` function is ignoring all the warnings 
and errors and running the workflow by the arguments `warning.stop` and 
`error.stop`, respectively.

```{r runWF_force, eval=FALSE}
sal <- runWF(sal, force = TRUE, warning.stop = FALSE, error.stop = TRUE)
```

When the project was initialized by `SPRproject` function, it was created an 
environment for all objects created during the workflow execution. This 
environment can be accessed as follows:

```{r runWF_env, eval=FALSE}
viewEnvir(sal)
```

The workflow execution allows to save this environment for future recovery:

```{r runWF_saveenv, eval=FALSE}
sal <- runWF(sal, saveEnv = TRUE)
```

### Workflow status

To check the summary of the workflow, we can use:

```{r show_statusWF, eval=TRUE}
sal
```

To access more details about the workflow instances, we can use the `statusWF` method:

```{r statusWF, eval=TRUE}
statusWF(sal)
```

## Visualize workflow

_`systemPipeR`_ workflows instances can be visualized with the `plotWF` function.

This function will make a plot of selected workflow instance and the following 
information is displayed on the plot:

    - Workflow structure (dependency graphs between different steps); 
    - Workflow step status, *e.g.* `Success`, `Error`, `Pending`, `Warnings`; 
    - Sample status and statistics; 
    - Workflow timing: running duration time. 

If no argument is provided, the basic plot will automatically detect width, 
height, layout, plot method, branches, _etc_. 

```{r, eval=TRUE}
plotWF(sal, show_legend = TRUE, width = "80%", rstudio = TRUE)
```

For more details about the `plotWF` function, please see [here](#plotWF).

## Generation of technical report

_`systemPipeR`_ compiles all the workflow execution logs in one central location, 
making it easier to check any standard output (`stdout`) or standard error 
(`stderr`) for any command-line tools used on the workflow or the R code `stdout`.
Also, the workflow plot is appended at the beginning of the report, making it 
easier to click on the respective step.

```{r, eval=FALSE}
sal <- renderLogs(sal)
```

## Parallelization on clusters

This section of the tutorial provides an introduction to the usage of the 
_`systemPipeR`_ features on a cluster.

The computation can be greatly accelerated by processing many files 
in parallel using several compute nodes of a cluster, where a scheduling/queuing
system is used for load balancing. For this the `clusterRun` function submits 
the computing requests to the scheduler using the run specifications
defined by `runWF`. 

A named list provides the computational resources. By default, it can be defined
the upper time limit in minutes for jobs before they get killed by the scheduler, 
memory limit in Mb, number of `CPUs`, and number of tasks. 

The number of independent parallel cluster processes is defined under the
`Njobs` argument. The following example will run one process in parallel using 
for each 4 CPU cores. If the resources available on a cluster allow running all 
the processes simultaneously, then the shown sample submission will utilize in 
total four CPU cores (`NJobs * ncpus`). Note, `clusterRun` can be used
with most queueing systems as it is based on utilities from the _`batchtools`_ 
package which supports the use of template files (_`*.tmpl`_) for defining the 
run parameters of different schedulers. To run the following code, one needs to 
have both a `conf file` (see _`.batchtools.conf.R`_ samples [here](https://mllg.github.io/batchtools/)) 
and a template file (see _`*.tmpl`_ samples [here](https://github.com/mllg/batchtools/tree/master/inst/templates)) 
for the queueing available on a system. The following example uses the sample 
`conf` and `template` files for the `Slurm` scheduler provided by this package.  

```{r clusterRun, eval=FALSE}
library(batchtools)
resources <- list(walltime=120, ntasks=1, ncpus=4, memory=1024)
sal <- clusterRun(sal, FUN = runWF, 
                  more.args = list(),
                  conffile=".batchtools.conf.R", 
                  template="batchtools.slurm.tmpl",
                  Njobs=1, runid="01", resourceList=resources)
```

Note: The example is submitting the jog to `short` partition. If you desire to 
use a different partition, please adjust accordingly (`batchtools.slurm.tmpl`).

## Exported the workflow 

_`systemPipeR`_ workflow management system allows to translate and export the 
workflow build interactively to R Markdown format or an executable bash script.
This feature advances the reusability of the workflow, as well as the flexibility
for workflow execution.

### R Markdown file

`sal2rmd` function takes an `SYSargsList` workflow container and translates it to 
SPR workflow template R markdown format. This file can be imported with the 
`importWF` function, as demonstrated above.

```{r, eval=FALSE}
sal2rmd(sal)
```

### Bash script

`sal2bash` function takes an `SYSargsList` workflow container and translates 
it to an executable bash script, so one can run the workflow without loading 
`SPR` or using an R console.


```{r, eval=FALSE}
sal2bash(sal)
```

It will be generated on the project root an executable bash script, called by
default the `spr_wf.sh`. Also, a directory `./spr_wf` will be created and store 
all the R scripts based on the workflow steps. Please note that this function will 
"collapse" adjacent R steps into one file as much as possible.

## Project Resume and Restart

If you desire to resume or restart a project that has been initialized in the past, 
`SPRproject` function allows this operation.

With the resume option, it is possible to load the `SYSargsList` object in R and 
resume the analysis. Please, make sure to provide the `logs.dir` location, and the 
corresponded `YAML` file name.
The current working directory needs to be in the project root directory.

```{r SPR_resume, eval=FALSE}
sal <- SPRproject(resume = TRUE, logs.dir = ".SPRproject", 
                  sys.file = ".SPRproject/SYSargsList.yml") 
```

If you choose to save the environment in the last analysis, you can recover all 
the files created in that particular section. `SPRproject` function allows this 
with `load.envir` argument. Please note that the environment was saved only with
you run the workflow in the last section (`runWF()`).

```{r resume_load, eval=FALSE}
sal <- SPRproject(resume = TRUE, load.envir = TRUE) 
```

After loading the workflow at your current section, you can check the objects 
created in the old environment and decide if it is necessary to copy them to the
current environment.

```{r envir, eval=FALSE}
viewEnvir(sal)
copyEnvir(sal, list="plot", new.env = globalenv())
```

This option will keep all previous logs in the folder; however, if you desire to 
clean the execution history and restart the workflow, the `restart=TRUE` option
can be used.

```{r restart_load, eval=FALSE}
sal <- SPRproject(restart = TRUE, overwrite = TRUE, load.envir = FALSE) 
```

The last and more drastic option from `SYSproject` function is to overwrite the
logs and the workflow. This option will delete the hidden folder and the 
information on the `SYSargsList.yml` files. This will not delete any parameter
file nor any results it was created in previous runs. Please use with caution. 

```{r SPR_overwrite, eval=FALSE}
sal <- SPRproject(overwrite = TRUE) 
```

## Showcase RNA-Seq workflow

If you desire to import and run a RNA-seq pipeline, please follow:

```{r project_import, eval=FALSE}
systemPipeRdata::genWorkenvir(workflow = "rnaseq")
setwd("rnaseq")
sal <- SPRproject() 
sal <- importWF(sal, file_path = "systemPipeRNAseq_importWF.Rmd", verbose = FALSE)
sal <- runWF(sal)
plotWF(sal, rstudio = TRUE)
sal <- renderLogs(sal)
```

## Overview of *systemPipeShiny* core functionalities

**<span style="color:#5DA7D6;">s</span>ystem<span
style="color:#5DA7D6;">P</span>ipe<span style="color:#5DA7D6;">S</span>hiny**
(SPS) extends the widely used [systemPipeR](/spr/) 
(SPR) workflow
environment with a versatile graphical user interface provided by a [Shiny
App](https://shiny.rstudio.com). This allows non-R users, such as
experimentalists, to run many systemPipeR's workflow designs, control, and
visualization functionalities interactively without requiring knowledge of R.
Most importantly, `SPS` has been designed as a general purpose framework for
interacting with other R packages in an intuitive manner. Like most Shiny Apps,
SPS can be used on both local computers as well as centralized server-based
deployments that can be accessed remotely as a public web service for using
SPR's functionalities with community and/or private data. The framework can
integrate many core packages from the R/Bioconductor ecosystem. Examples of
SPS' current functionalities include: 

- A default interactive workflow module to 
create experimental designs, visualize and customize workflow topologies with previews, and 
programming free workflow execution within the application. 
- An interactive module with extensive plot options to visualize downstream analysis of a RNA-Seq workflow.
- A quick ggplot module to make all variety of scientific plots from any user defined 
tabular data. 
- An extendable set of visualization functionalities makes it easy to design 
custom Shiny Apps under SPS framework without any knowledge of Shiny. 
- A 'Canvas Workbench' to manage complex visual results. It allows users to 
organize and to compare plots in an efficient manner combined
with a session screenshot feature to edit scientific and publishable figures. 
- Three other supporting packages to help all users from beginners and advanced developers 
to extend under current SPS framework or on their own visualization apps. 

## Demo
View our online demo app:

| Type and link| option changed | notes |
| --- | --- | --- |
| [Default full installation{blk}](https://tgirke.shinyapps.io/systemPipeShiny/) | [See installation](#installation) | full app |
| [Minimum installation{blk}](https://tgirke.shinyapps.io/systemPipeShiny/) | [See installation](#installation) | no modules installed |
| [Login enabled{blk}](https://tgirke.shinyapps.io/systemPipeShiny_loading/) | `login_screen = TRUE; login_theme = "empty"` | no modules installed |
| [Login and login themes{blk}](https://tgirke.shinyapps.io/systemPipeShiny_loading_theme/) | `login_screen = TRUE; login_theme = "random"` | no modules installed |
| [App admin page{blk}](https://tgirke.shinyapps.io/systemPipeShiny_loading/?admin) | `admin_page = TRUE` | or simply add "?admin" to the end of URL of demos |

For the login required demos, the app account name is **"user"** password **"user"**.

For the admin panel login, account name **"admin"**, password **"admin"**.

**Please DO NOT delete or change password when you are using the admin features.**
_shinyapp.io_ will reset the app once a while, but this will affect other people 
who are trying the demo simultaneously. 

# Version information

```{r sessionInfo}
sessionInfo()
```

```{r cleaning, eval=TRUE, include=FALSE}
print(getwd())
if (file.exists(".SPRproject")) unlink(".SPRproject", recursive = TRUE)
setwd("../")
if (file.exists("bioc2021")) unlink("bioc2021", recursive = TRUE)
## NOTE: Removing previous project create in the quick starts section
```

# Funding

This project is funded by NSF award [ABI-1661152](https://www.nsf.gov/awardsearch/showAward?AWD_ID=1661152). 

# References
